#!/usr/bin/env python3
import streamlit as st
import pandas as pd
import plotly.express as px
import json
import requests
from datetime import datetime
import numpy as np

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="AIæ¨¡å‹ç»“æœè¡¨æ ¼",
    page_icon="ğŸ“Š",
    layout="wide"
)

# GitHub Raw URL - è¿™é‡Œä½¿ç”¨ä½ çš„ä»“åº“åœ°å€
GITHUB_RAW_URL = "https://raw.githubusercontent.com/HZxCzar/FrontEnd/main/cache.json"

@st.cache_data(ttl=300)
def load_data_from_github():
    try:
        response = requests.get(GITHUB_RAW_URL, timeout=10)
        if response.status_code == 200:
            return response.json()
        else:
            st.error(f"æ— æ³•è·å–æ•°æ®ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return {"results": []}
    except Exception as e:
        st.error(f"åŠ è½½æ•°æ®é”™è¯¯: {str(e)}")
        return {"results": []}

def parse_test_results(test_string):
    if not test_string:
        return {}
    
    lines = test_string.strip().split('\n')
    if len(lines) < 2:
        return {}
    
    try:
        headers = [h.strip() for h in lines[0].split(',')]
        values = [v.strip() for v in lines[1].split(',')]
        
        result = {}
        for header, value in zip(headers, values):
            if header == 'Model':
                continue
            try:
                result[header] = float(value)
            except ValueError:
                result[header] = value
        return result
    except:
        return {}

def get_min_loss(train_string):
    if not train_string:
        return None
    
    lines = train_string.strip().split('\n')
    if len(lines) < 2:
        return None
    
    try:
        loss_values = []
        parts = lines[1].split(',')[1:]  # è·³è¿‡ç¬¬ä¸€ä¸ªæ ‡ç­¾
        
        for part in parts:
            try:
                loss_values.append(float(part.strip()))
            except ValueError:
                continue
        
        return min(loss_values) if loss_values else None
    except:
        return None

def main():
    st.title("ğŸ“Š AIæ¨¡å‹ç»“æœè¡¨æ ¼")
    st.markdown("æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„æœ€ä½lossã€12ä¸ªæµ‹è¯•é›†ç»“æœå’Œæµ‹è¯•é›†å‡å€¼")
    st.markdown("---")
    
    # åŠ è½½æ•°æ®
    with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®..."):
        cache = load_data_from_github()
    
    results = cache.get("results", [])
    
    if not results:
        st.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
        st.info("è¯·å…ˆè¿è¡Œæ•°æ®æ›´æ–°è„šæœ¬: python update_cache.py")
        return
    
    # åˆ›å»ºè¡¨æ ¼æ•°æ®
    table_data = []
    
    for result in results:
        if not result.get('name'):
            continue
            
        row = {'æ¨¡å‹åç§°': result['name']}
        
        # æœ€ä½loss
        min_loss = get_min_loss(result.get('train', ''))
        row['æœ€ä½Loss'] = f"{min_loss:.4f}" if min_loss else 'N/A'
        
        # æµ‹è¯•ç»“æœ
        test_results = parse_test_results(result.get('test', ''))
        
        # 12ä¸ªæµ‹è¯•é›†
        test_datasets = [
            'ARC Challenge', 'ARC Easy', 'BoolQ', 'FDA', 'HellaSwag', 
            'LAMBDA OpenAI', 'OpenBookQA', 'PIQA', 'Social IQA', 
            'SQuAD Completion', 'SWDE', 'WinoGrande'
        ]
        
        test_values = []
        for dataset in test_datasets:
            if dataset in test_results:
                value = test_results[dataset]
                row[dataset] = f"{value:.3f}" if isinstance(value, (int, float)) else value
                if isinstance(value, (int, float)):
                    test_values.append(value)
            else:
                row[dataset] = 'N/A'
        
        # æµ‹è¯•é›†å‡å€¼
        if test_values:
            row['æµ‹è¯•é›†å‡å€¼'] = f"{np.mean(test_values):.4f}"
        elif 'Average' in test_results:
            row['æµ‹è¯•é›†å‡å€¼'] = f"{test_results['Average']:.4f}"
        else:
            row['æµ‹è¯•é›†å‡å€¼'] = 'N/A'
        
        table_data.append(row)
    
    if not table_data:
        st.error("æ²¡æœ‰å¯æ˜¾ç¤ºçš„æ•°æ®")
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ”¢ æ¨¡å‹æ€»æ•°", len(table_data))
    with col2:
        st.metric("ğŸ“Š è®°å½•æ•°", len(results))
    with col3:
        last_update = cache.get("last_update", "æœªçŸ¥")
        if last_update != "æœªçŸ¥":
            try:
                last_update = datetime.fromisoformat(last_update).strftime('%m-%d %H:%M')
            except:
                pass
        st.metric("ğŸ•’ æœ€åæ›´æ–°", last_update)
    
    # æ˜¾ç¤ºè¡¨æ ¼
    df = pd.DataFrame(table_data)
    st.subheader("ğŸ“‹ æ¨¡å‹ç»“æœè¡¨æ ¼")
    st.dataframe(df, use_container_width=True, hide_index=True)
    
    # ä¸‹è½½æŒ‰é’®
    csv = df.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½CSV",
        data=csv,
        file_name=f'model_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.csv',
        mime='text/csv'
    )

if __name__ == "__main__":
    main()
